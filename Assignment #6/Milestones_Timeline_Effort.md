# Milestones List
**Note:** Since there is only one group member, all work will be done by David Earnest 
1. **Working solution to collect training dataset:** We will develop and implement an approach to create the training dataset.
2. **Behavior Cloning Working**: We will develop and implement an appoarch to clone the behavioral policy.
3. **Method Implemented and Working:** We will implement or method.
4. **Results Collection:** We will collect results with our new method.
5. **Comparision:** We will compare our method to other approaches.

# Timeline
| Task | Start Date | Completion Date |
|-|-|-|
| Research reinforcement learning theory | 9/10/23 | 10/1/23 |
| Research offline reinforcement learning | 9/10/23 | 10/1/23 |
| Research constrained optimization | 9/10/23 | 10/1/2 |
| Design a new algorithm for offline reinforcement learning | 9/20/23 | 9/28/23 |
| Justify algorithm (either intuitively or theoretically) | 9/20/23 | 9/28/23 |
| Milestone #1 | | 12/11/24 |
| Milestone #2 | | 12/11/24 |
| Implement the new algorithm in code | 12/11/23 | 1/1/24 |
| Milestone #3 | | 1/1/24 |
| Choose toy problem to validate | 9/10/23 | 10/15/23 |
| Validate the correctness of the algorithm on toy problem, i.e., debug and ensure the algorithm works | 12/11/23 | 1/1/24 |
| Select benchmark to evaluate the new algorithm | 1/1/24 | 1/15/24 |
| Tune the hyperparameters of the algorithm | 1/15/24 | 2/1/24 |
| Determine the algorithms performance on the benchmark | 1/15/24 | 2/1/24 |
| Milestone #4 | | 2/1/24 |
| Select offline reinforcement learning algorithms to compare our new algorithm to| 2/1/24 | 2/10/24 |
| Collect comparison results | 2/10/24 | 3/1/24 |
| Write up the results | 3/1/24 | 4/1/22 |
| Milestone #5 | | 4/1/24 |

# Effort Matrix
| Task | Effort |
|-|-|
| Research reinforcement learning theory | 5% |
| Research offline reinforcement learning | 5% |
| Research constrained optimization | 5% |
| Design a new algorithm for offline reinforcement learning | 10% |
| Justify algorithm (either intuitively or theoretically) | 5% |
| Implement the new algorithm in code | 15% |
| Choose toy problem to validate | 5% |
| Validate the correctness of the algorithm on toy problem, i.e., debug and ensure the algorithm works | 10% |
| Select benchmark to evaluate the new algorithm | 5% |
| Tune the hyperparameters of the algorithm | 5% |
| Determine the algorithms performance on the benchmark | 5% |
| Select offline reinforcement learning algorithms to compare our new algorithm to| 5% |
| Collect comparison results| 15% |
| Write up the results| 5% |