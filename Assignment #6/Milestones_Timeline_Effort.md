
# Milestones
**Note:** Since there is only one group member, all work will be done by David Earnest 

## Working solution to collect training dataset
Upon reaching this milestone I will have an policy which can collect the collect the training dataset which I will use to train my offline RL agent.

Completion Date: TBD

Deliverables
- Behavioral policy that can collect the training dataset
- Training dataset collected with the behavioral policy

## Behavior Cloning Working
After completing this milestone, I will have cloned the behavioral policy. I will used this clone policy later in the algorithm.

Completion Date: TBD

Deliverables
- Cloned policy
- Neural network and training design
- Network weights saved

## Method Implemented and Working
This milestone concludes the completion of my algorithms implmentation. I will have a working, tested implementation of my algorithm.

Completion Date: TBD

Deliverables
- New algorithm fully implemented in code
-  Network weights saved

## Results Collection
This milestone concludes the collection of results with my new methods. I will collect data and create plots that let me evaluate the performance of my algorithm.

Completion Date: TBD

Deliverables
- Testing results
- Plots

## Comparision
This milestone concludes the comparision of my algorithm with existing methods.

Completion Date: TBD

Deliverables
- Other current methods implemented
- Plots comparing methods

# Timeline
| Task | Start Date | Completion Date |
|-|-|-|
| Research reinforcement learning theory | 9/10/23 | 10/1/23 |
| Research offline reinforcement learning | 9/10/23 | 10/1/23 |
| Research constrained optimization | 9/10/23 | 10/1/2 |
| Design a new algorithm for offline reinforcement learning | 9/20/23 | 9/28/23 |
| Justify algorithm (either intuitively or theoretically) | 9/20/23 | 9/28/23 |
| Milestone #1 | | 12/11/24 |
| Milestone #2 | | 12/11/24 |
| Implement the new algorithm in code | 12/11/23 | 1/1/24 |
| Milestone #3 | | 1/1/24 |
| Choose toy problem to validate | 9/10/23 | 10/15/23 |
| Validate the correctness of the algorithm on toy problem, i.e., debug and ensure the algorithm works | 12/11/23 | 1/1/24 |
| Select benchmark to evaluate the new algorithm | 1/1/24 | 1/15/24 |
| Tune the hyperparameters of the algorithm | 1/15/24 | 2/1/24 |
| Determine the algorithms performance on the benchmark | 1/15/24 | 2/1/24 |
| Milestone #4 | | 2/1/24 |
| Select offline reinforcement learning algorithms to compare our new algorithm to| 2/1/24 | 2/10/24 |
| Collect comparison results | 2/10/24 | 3/1/24 |
| Write up the results | 3/1/24 | 4/1/22 |
| Milestone #5 | | 4/1/24 |

# Effort Matrix
| Task | Effort |
|-|-|
| Research reinforcement learning theory | 5% |
| Research offline reinforcement learning | 5% |
| Research constrained optimization | 5% |
| Design a new algorithm for offline reinforcement learning | 10% |
| Justify algorithm (either intuitively or theoretically) | 5% |
| Implement the new algorithm in code | 15% |
| Choose toy problem to validate | 5% |
| Validate the correctness of the algorithm on toy problem, i.e., debug and ensure the algorithm works | 10% |
| Select benchmark to evaluate the new algorithm | 5% |
| Tune the hyperparameters of the algorithm | 5% |
| Determine the algorithms performance on the benchmark | 5% |
| Select offline reinforcement learning algorithms to compare our new algorithm to| 5% |
| Collect comparison results| 15% |
| Write up the results| 5% |